{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a705a7",
   "metadata": {},
   "source": [
    "# CS5914 Machine Learning Algorithms\n",
    "## Assignment 3 \n",
    "##### Credits: 35% of coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26dde6",
   "metadata": {},
   "source": [
    "## Aims\n",
    "\n",
    "The objectives of this assignment are:\n",
    "\n",
    "* deepen your understanding of neural networks\n",
    "* deepen your understanding of supervised and self-supervised learning\n",
    "* deepen your understanding of encoder and decoder networks \n",
    "* gain experience in implementing back propagation and optimisers\n",
    "* gain experience in implementing pre-trained deep neural networks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1569e8f",
   "metadata": {},
   "source": [
    "# Task 1. Supervised Learning with Neural Networks\n",
    "\n",
    "Task 1 will involve implementing a simple Neural Network that classifies 2d gray scale images and training it using backpropagation and an optimiser."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "25f497264e75e432"
  },
  {
   "cell_type": "markdown",
   "id": "f662dc6a",
   "metadata": {},
   "source": [
    "### Set-up\n",
    "\n",
    "Load required packages (you can only use the imported packages for this task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931f3a91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T13:18:37.917807900Z",
     "start_time": "2024-08-03T13:18:37.083583900Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # use this to plot training charts at your discretion\n",
    "import numpy as np \n",
    "import glob # used esclusively to load files from folders\n",
    "from PIL import Image # used to manipulate images and prepare data (you should not need it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be8be4",
   "metadata": {},
   "source": [
    "### Task 1.1 Implement a simple ReLu activation neuron, a premade implementation of a Linear Layer that goes with it is provided to you. For each of these two components describe the role of each method and how the two components work together.\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549bf9fb",
   "metadata": {},
   "source": [
    "#### You can follow the template below, but feel free to start from scratch with your own design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad377c",
   "metadata": {},
   "source": [
    "#### ReLu Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3303e89b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T13:18:38.223341600Z",
     "start_time": "2024-08-03T13:18:37.910749100Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "clip() missing 1 required positional argument: 'a_max'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;43;01mclass\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43;01mReLu\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Implement a variable that will store the input during the forward pass\u001B[39;49;00m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minput_dim\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m, in \u001B[0;36mReLu\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mReLu\u001B[39;00m:\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;66;03m# Implement a variable that will store the input during the forward pass\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_dim, output_dim\u001B[38;5;241m=\u001B[39m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m):\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput  \u001B[38;5;241m=\u001B[39m input_dim\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m output_dim\n",
      "\u001B[1;31mTypeError\u001B[0m: clip() missing 1 required positional argument: 'a_max'"
     ]
    }
   ],
   "source": [
    "class ReLu:\n",
    "    # Implement a variable that will store the input during the forward pass\n",
    "    def __init__(self, input_dim, output_dim=np.clip(input, 0)):\n",
    "        self.input  = input_dim\n",
    "        self.output = output_dim\n",
    "        \n",
    "        \n",
    "    # Implement the forward pass of the ReLU\n",
    "    def forward(self, x):\n",
    "        return self.__init__(x)\n",
    "\n",
    "    # Implement the gradient for the backward pass\n",
    "    def backward(self, gradient_output):\n",
    "        return (self.input >0)*gradient_output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806231b2",
   "metadata": {},
   "source": [
    "#### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb81d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T13:18:38.240503800Z",
     "start_time": "2024-08-03T13:18:38.225726200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Layer. You can use it as is.\n",
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim) # 'He et al' init strategy\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        self.input = None\n",
    "        self.grad_weights = None\n",
    "        self.grad_biases = None\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.grad_weights\n",
    "    \n",
    "    def get_biases(self):\n",
    "        return self.grad_biases\n",
    "    \n",
    "    def set_weights(self, weight):\n",
    "        self.weights = weight\n",
    "    \n",
    "    def set_biases(self, bias):\n",
    "        self.biases = bias\n",
    "    \n",
    "    def get_core_props(self):\n",
    "        return np.array([self.get_weights(), self.get_biases()])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_weights = np.dot(self.input.T, grad_output)\n",
    "        self.grad_biases = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        return np.dot(grad_output, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a39ac",
   "metadata": {},
   "source": [
    "### Task 1.2 Implement a simple feed forward neural network, its loss function and a method for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c7d43",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.229210200Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers = []):\n",
    "        self.layers = layers()\n",
    "        \n",
    "    # a simple function that can be used to add \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Implement a function that applies the input x iteratively to each layer and return the final output\n",
    "    def forward(self, x):\n",
    "        y_means = x \n",
    "        for layer in self.layers:\n",
    "            y_means = layer.forward(y_means)\n",
    "        return y_means\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        results = self.forward(x_test)\n",
    "        return results\n",
    "        \n",
    "\n",
    "    # Implement a function that iteratively backpropagate the gradients. Remember to start from the last layer.\n",
    "    def backward(self, gradient_output):\n",
    "        for layer in self.layers:\n",
    "            gradient_output = layer.backward(gradient_output)\n",
    "        return gradient_output\n",
    "    \n",
    "    def update_props(self, updated_params):\n",
    "        for i in range(len(updated_params)):\n",
    "            upd_weight, upd_bias = updated_params[i]\n",
    "            self.layers[i].set_weights(upd_weight)\n",
    "            self.layers[i].set_bias(upd_bias)\n",
    "    \n",
    "    # implement a function that returns all the parameters for each layer (both weights and biases) and the gradients\n",
    "    def parameters_and_gradients(self):\n",
    "        layer_props = [item.get_core_props() for item in self.layers]\n",
    "        return  np.array(layer_props)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70932133",
   "metadata": {},
   "source": [
    "#### Implement the categorical cross entropy loss function:\n",
    "\n",
    "\n",
    "$-\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$\n",
    "* M - number of classes\n",
    "\n",
    "* log - the natural log\n",
    "\n",
    "* y - binary indicator (0 or 1) if class label c is the correct classification for observation o\n",
    "\n",
    "* p - predicted probability observation o is of class c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935b07c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.230366500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the categorical cross entropy loss function\n",
    "def cce_loss(y_true, y_pred):\n",
    "    y_pred = softmax(y_pred)\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46575bd4",
   "metadata": {},
   "source": [
    "In order to turn the output of the neural network's last layer into probabilities a softmax function should be used.\n",
    "An implementation of the function and its derivative is provided for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab975b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.231452100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Softmax \n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# the derivative is that simple due to a combination of choices, the one hot encoding and the use of cross entropy as loss function.\n",
    "def derivative_cross_entropy_softmax(preds, labels):\n",
    "    size = labels.shape[0]\n",
    "    return (preds - labels) / size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c196c1",
   "metadata": {},
   "source": [
    "### Task1.3 Implement the optimiser based on gradient descent described by the formula:\n",
    "$ \\theta = \\theta â€“ \\eta \\cdot \\nabla_\\theta J(\\theta)$\n",
    "\n",
    "* $\\theta$ is a parameter that will be updated\n",
    "* $\\eta$ is the learning rate \n",
    "* $\\nabla_\\theta J(\\theta)$ is the gradient of the loss function $J$ with respect to $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c226c3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.233664600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement a function that iterates over the parameters and update them based on the learning rate and the gradients\n",
    "def sgd(parameters_and_gradients, lr):\n",
    "    \n",
    "    gradients, bias  = parameters_and_gradients\n",
    "    for i in range(len(parameters_and_gradients)):\n",
    "        upd_grad = gradients[i]- (lr * gradients[i]) \n",
    "        upd_bias = bias[i]- (lr * bias[i])\n",
    "        parameters_and_gradients[i][0]= upd_grad\n",
    "        parameters_and_gradients[i][1]= upd_bias\n",
    "        \n",
    "    return parameters_and_gradients\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bcf83",
   "metadata": {},
   "source": [
    "### Task1.4 Implement the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9821b82",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.235892100Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(network:  NeuralNetwork, X_train, Y_train, batch_size, epochs, learning_rate):\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "            \n",
    "    for epoch in range(epochs):\n",
    "        # shuffle data on each epoch\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        Y_train_shuffled = Y_train[indices]\n",
    "        loss = 0\n",
    "        # a guarded batching loop is provided for your convenience\n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            end = min(end,n_samples)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            Y_batch = Y_train_shuffled[start:end]\n",
    "            results= network.forward(X_batch)\n",
    "            loss += cce_loss(results, Y_batch)\n",
    "            grad_loss = results - Y_batch\n",
    "            network.backward(grad_loss)\n",
    "            \n",
    "            weights, bias = sgd(network.parameters_and_gradients(), learning_rate)\n",
    "            network.update_props((weights, bias))\n",
    "            # Implement the necessary missing operations:\n",
    "            # - Predict classes for the batch of observations (remember to use softmax on the NN output)\n",
    "            # - Compute the loss between predictions and true lables\n",
    "            # - Compute the gradient of the softmax operation\n",
    "            # - Backpropagate the gradients thoughout the network\n",
    "            # - Use the gradient descent function to update the parameters\n",
    "        loss = loss / n_batches\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "    return network\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d555f24",
   "metadata": {},
   "source": [
    "### Task1.5 Instantiate a Neural Network and train it on the provided dataset, then present and evaluate the results obtained by using the Neural Network on the provided test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf84476",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.237041200Z"
    }
   },
   "outputs": [],
   "source": [
    "#Unzip the folder you have been provided with in the same folder as this jupyter notebook. This code will prepare training and test sets.\n",
    "filelist = glob.glob('dataset1/train/*.png')\n",
    "train_images_np = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
    "classes_labels_train = np.array([int(fname[-5:-4]) for fname in filelist]) #labels are encoded in the last digit of the name\n",
    "# encoding the training set labels into one hot encoding\n",
    "train_labels = np.zeros((classes_labels_train.size, classes_labels_train.max()+1), dtype=int)\n",
    "train_labels[np.arange(classes_labels_train.size),classes_labels_train] = 1 \n",
    "\n",
    "\n",
    "filelist = glob.glob('dataset1/test/*.png')\n",
    "images_np_test = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
    "classes_labels_test = np.array([int(fname[-5:-4]) for fname in filelist]) #labels are encoded in the last digit of the name\n",
    "# encoding the test set labels into one hot encoding\n",
    "test_labels = np.zeros((classes_labels_test.size, classes_labels_test.max()+1), dtype=int)\n",
    "test_labels[np.arange(classes_labels_test.size),classes_labels_test] = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d093980",
   "metadata": {},
   "source": [
    "#### Instantiate the network and set the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627f0f4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.240503800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise Network \n",
    "network = NeuralNetwork([Linear, ReLu, Linear])\n",
    "# add an input layer, at least a few hidden layers and an output layer to the network\n",
    "\n",
    "\n",
    "# Training Parameters - set the appropriate parameters\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18c16f",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6514d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T13:18:38.251729Z",
     "start_time": "2024-08-03T13:18:38.242732900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the Network\n",
    "network = train(network, train_images_np, train_labels, batch_size, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca83f1",
   "metadata": {},
   "source": [
    "#### Test the network and present the results. This can be aided by plots. Critically evaluate the results. You are encourage to try different parameters and setting, and searching for the smallest most efficient network that can learn the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8338e8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.246105200Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    score = len([y_true==y_pred])/ len(y_pred)\n",
    "    return score \n",
    "\n",
    "def plot_graph(test_data, gen_results, true_labels):\n",
    "    plt.clf()\n",
    "    plt.plot(test_data, gen_results, c= true_labels)\n",
    "    plt.show()\n",
    "# test the network by using the forward function only.\n",
    "results = network.predict(images_np_test)\n",
    "print(accuracy_score(results, test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13ff12-10f8-47d6-8863-1aac23b300ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Task 2 Self-Supervised Learning with Convolutional Autoencoders\n",
    "Task 2 will involve constructing a Convolutional Autoencoder and training it on a collection of unlabelled RGB images in order to create a 2D latent space. After visualising how the inputs are encoded into 2d vectors by the network the pre-trained encoder will be extended with a dense classification layer that will be trainined on a smaller set of labelled images.\n",
    "\n",
    "In this task you are allowed to use torch to build your encoder-decoder network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76162536",
   "metadata": {},
   "source": [
    "### Set-up\n",
    "\n",
    "Load required packages (you can only use the imported packages for this task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae96f4a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.248416300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import glob # used esclusively to load files from folders\n",
    "from PIL import Image # used to manipulate images and prepare data (you should not need it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6819f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.250620400Z"
    }
   },
   "outputs": [],
   "source": [
    "filelist = glob.glob('dataset2/unlabelled/*.png') # Use these files to pre-train the autoencoder\n",
    "filelist2 = glob.glob('dataset2/train/*.png') # Use these files to train the classifier\n",
    "filelist3 = glob.glob('dataset2/test/*.png') # Use these files to test the classifier\n",
    "unlabelled_images = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
    "\n",
    "train_images = np.array([np.array(Image.open(fname)) for fname in filelist2])\n",
    "train_labels = np.array([int(fname[-5:-4]) for fname in filelist2])\n",
    "\n",
    "test_images = np.array([np.array(Image.open(fname)) for fname in filelist3])\n",
    "test_labels = np.array([int(fname[-5:-4]) for fname in filelist3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef99e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T13:18:38.263213700Z",
     "start_time": "2024-08-03T13:18:38.252731600Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, numpy_array):\n",
    "        self.data = torch.tensor(numpy_array, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0  # Convert to torch.Tensor and normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f2aca",
   "metadata": {},
   "source": [
    "### Task 2.1 set up an Autoencoder using torch convolutional layers. Make the encoder and decoder at least a few layers deep each. \n",
    "#### Further:\n",
    "* Describe the 2D convolution operation, provide the formula used by the library and the meaning of the parameters.\n",
    "* Describe each layer you have decided to use beyond that, provide the underlying formulas and the meaning of the parameters.\n",
    "* If one type of layer is used multiple times one explaination is sufficient.\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d58ea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.255358700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Auto Encoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1), # Input: (3, 64, 64), Output: (16, 32, 32)\n",
    "            ## Implement here your architecture using torch layers\n",
    "        )\n",
    "\n",
    "        # Remember that the last layer of the encoder and the first layer of the decoder should be the same \n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            ## Implement here your architecture using torch layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2c6bc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.258727900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_autoencoder(model, dataloader, loss_function, optimiser, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for imgs in dataloader:\n",
    "            optimiser.zero_grad()\n",
    "            _ , outputs = model(imgs) # here the encoded values are not used, but you will need it later\n",
    "            loss = loss_function(outputs, imgs)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f772e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.262092300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters (Adjust as needed)\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiation of the model\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# loss function and optimiser\n",
    "loss_function = nn.MSELoss()\n",
    "optimisation_alg = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Dataset Preparation\n",
    "imageSize = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((imageSize, imageSize)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "custom_dataset = CustomDataset(unlabelled_images)\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# training\n",
    "train_autoencoder(autoencoder, dataloader,loss_function,optimisation_alg, epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70692235",
   "metadata": {},
   "source": [
    "### Task 2.2 Use the pretrained encoder to extract the encoded 2D vectors and plot them using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac986f60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T13:18:38.450120500Z",
     "start_time": "2024-08-03T13:18:38.264313600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the extraction of the encoded input.\n",
    "def latent_space(autoencoder, dataloader):\n",
    "    # make sure to use the autoencoder in evaluation mode. Do not compute the gradients.\n",
    "    latent_spaces = []\n",
    "    # use the autoencoder to produce the latent spaces. Here is where you have to get the encoded values instead of the outputs\n",
    "\n",
    "    return latent_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853a928",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.266483100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting function. If the training is succesful similar images should be clustered together.\n",
    "def plot_latent_space(latent_space):\n",
    "\n",
    "    def getImage(path, zoom=1):\n",
    "        return OffsetImage(plt.imread(path), zoom=0.3)\n",
    "\n",
    "    x = latent_space[:,0]\n",
    "    y = latent_space[:,1]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    ax.scatter(x, y)\n",
    "\n",
    "    for x0, y0, path in zip(x, y, filelist): #if you shuffled the data then you must update the filelist array accordingly\n",
    "        ab = AnnotationBbox(getImage(path), (x0, y0), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "    plt.axis('off')\n",
    "\n",
    "coordinates = latent_space(autoencoder, dataloader)\n",
    "plot_latent_space(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083aa89",
   "metadata": {},
   "source": [
    "### Task 2.3 (Advanced) Use the pretrained encoder and join the output of the encoder to a Dense Feed Forward Layer followed by an Output Layer with as many neurons as classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80595ad8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.267565900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the missing functionalities below\n",
    "class PretrainedAEClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_encoder, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use the pretrained encoder\n",
    "        self.encoder = pretrained_encoder\n",
    "        \n",
    "        # Freeze the encoder layers\n",
    "        for param in self.encoder.parameters():\n",
    "            ???\n",
    "            \n",
    "        # Add a new dense layer for classification\n",
    "        self.classifier = ???\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Use the encoder to extract features\n",
    "        ???\n",
    "        # Pass the encoded features through the classifier\n",
    "        ???\n",
    "        return class_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d171a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-03T13:18:38.269784400Z"
    }
   },
   "outputs": [],
   "source": [
    "# this extracts the encoder from the autoencoder\n",
    "encoder_model = autoencoder.encoder\n",
    "\n",
    "number_of_classes = ??\n",
    "\n",
    "# Create the classifier model using the pretrained encoder \n",
    "classifier_model = PretrainedAEClassifier(encoder_model,number_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbbbd4",
   "metadata": {},
   "source": [
    "#### Re-implement the training loop from Task2.1 but this time compute the loss function based on the labels of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d84f56",
   "metadata": {},
   "source": [
    "#### Use the trained classifier on the Test data provided. Briefly present and discuss the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b10cb5",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Hand in via Moodle: the completed jupyter notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422044c",
   "metadata": {},
   "source": [
    "## Marking\n",
    "Your submission will be marked as a whole. \n",
    "\n",
    "* to get a grade above 13, you are expected to finish at least Task 1 up to 1.5 to a good standard\n",
    "* to get a grade above 13 and up to 17, you are expected to answer Task 1 and Task 2.1-2.2 to a good standard\n",
    "* to achieve a grade of 17-18, you are expected to finish all tasks except Task 2.3 flawlessly \n",
    "* to get 18+, you are expected to attempt all questions flawlessly\n",
    "\n",
    "\n",
    "Marking is according to the standard mark descriptors published in the Student Handbook at:\n",
    "\n",
    "https://info.cs.st-andrews.ac.uk/student-handbook/learning-teaching/feedback.html#GeneralMarkDescriptors\n",
    "\n",
    "\n",
    "You must reference any external sources used. Guidelines for good academic practice are outlined in the student handbook at https://info.cs.st-andrews.ac.uk/student-handbook/academic/gap.html\n",
    "\n",
    "\n",
    "## Submission dates\n",
    "There are no fixed submission dates. Formal exam boards are held in January, May and September. Any provisional grades recorded on MMS are discussed by the Programme team and the External Examiner. Credits for a module can only be obtained after all the coursework has been discussed at an exam board, so students must submit work at least three weeks before the board date, giving time for grading and preparation of feedback.\n",
    "\n",
    "https://www.st-andrews.ac.uk/education/staff/assessment/reporting/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
